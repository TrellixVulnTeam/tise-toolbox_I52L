{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from easydict import EasyDict as eDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from temperature_scaling import ModelWithTemperature\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load validation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load(\"./data/coco_object_validation_feature_data.npz\")\n",
    "# data = np.load(\"./data/cub_validation_feature_data.npz\")\n",
    "# data = np.load(\"./data/image_net_validation_feature_data.npz\")\n",
    "# data = np.load(\"./data/tf_image_net_validation_feature_data.npz\")\n",
    "features = torch.Tensor(data[\"features\"])\n",
    "labels = torch.LongTensor(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return feat, label\n",
    "\n",
    "\n",
    "class InceptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence Calibration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = FeatureDataset(features, labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before temperature - NLL: 0.332, ECE: 0.255\n",
      "Optimal temperature: 0.2188338041305542\n",
      "After temperature - NLL: 0.0706715062, ECE: 0.0036738028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelWithTemperature(\n",
       "  (model): InceptionModel()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InceptionModel()\n",
    "model.eval()\n",
    "scaled_model = ModelWithTemperature(model, init_temp=0.23)\n",
    "scaled_model.cuda()\n",
    "scaled_model.set_temperature(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reliability Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, val_loader):\n",
    "    y_preds = torch.LongTensor([]).cuda()\n",
    "    y_trues = torch.LongTensor([]).cuda()\n",
    "    y_confs = torch.FloatTensor([]).cuda()\n",
    "\n",
    "    for features, y_true in val_loader:\n",
    "        features = features.cuda()\n",
    "        y_true = y_true.cuda()\n",
    "\n",
    "        y_prob = F.softmax(model(features), -1)\n",
    "        y_conf, y_pred = torch.max(y_prob, 1)\n",
    "        y_preds = torch.cat((y_preds, y_pred), 0)\n",
    "        y_trues = torch.cat((y_trues, y_true), 0)\n",
    "        y_confs = torch.cat((y_confs, y_conf), 0)\n",
    "\n",
    "    y_confs = y_confs.data.cpu().numpy()\n",
    "    y_preds = y_preds.data.cpu().numpy()\n",
    "    y_trues = y_trues.data.cpu().numpy()\n",
    "\n",
    "    return y_confs, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_bin(conf_thresh_lower, conf_thresh_upper, conf, pred, true):\n",
    "    \"\"\"\n",
    "    # Computes accuracy and average confidence for bin\n",
    "\n",
    "    Args:\n",
    "        conf_thresh_lower (float): Lower Threshold of confidence interval\n",
    "        conf_thresh_upper (float): Upper Threshold of confidence interval\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "\n",
    "    Returns:\n",
    "        (accuracy, avg_conf, len_bin): accuracy of bin, confidence of bin and number of elements in bin.\n",
    "    \"\"\"\n",
    "    filtered_tuples = [x for x in zip(pred, true, conf) if x[2] > conf_thresh_lower and x[2] <= conf_thresh_upper]\n",
    "    if len(filtered_tuples) < 1:\n",
    "        return 0, 0, 0\n",
    "    else:\n",
    "        correct = len([x for x in filtered_tuples if x[0] == x[1]])  # How many correct labels\n",
    "        len_bin = len(filtered_tuples)  # How many elements falls into given bin\n",
    "        avg_conf = sum([x[2] for x in filtered_tuples]) / len_bin  # Avg confidence of BIN\n",
    "        accuracy = float(correct) / len_bin  # accuracy of BIN\n",
    "        return accuracy, avg_conf, len_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECE(conf, pred, true, bin_size=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    Expected Calibration Error\n",
    "\n",
    "    Args:\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
    "\n",
    "    Returns:\n",
    "        ece: expected calibration error\n",
    "    \"\"\"\n",
    "\n",
    "    upper_bounds = np.arange(bin_size, 1 + bin_size, bin_size)  # Get bounds of bins\n",
    "\n",
    "    n = len(conf)\n",
    "    ece = 0  # Starting error\n",
    "\n",
    "    for conf_thresh in upper_bounds:  # Go through bounds and find accuracies and confidences\n",
    "        acc, avg_conf, len_bin = compute_acc_bin(conf_thresh - bin_size, conf_thresh, conf, pred, true)\n",
    "        ece += np.abs(acc - avg_conf) * len_bin / n  # Add weigthed difference to ECE\n",
    "\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_info(conf, pred, true, bin_size=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    Get accuracy, confidence and elements in bin information for all the bins.\n",
    "\n",
    "    Args:\n",
    "        conf (numpy.ndarray): list of confidences\n",
    "        pred (numpy.ndarray): list of predictions\n",
    "        true (numpy.ndarray): list of true labels\n",
    "        bin_size: (float): size of one bin (0,1)  # TODO should convert to number of bins?\n",
    "\n",
    "    Returns:\n",
    "        (acc, conf, len_bins): tuple containing all the necessary info for reliability diagrams.\n",
    "    \"\"\"\n",
    "\n",
    "    upper_bounds = np.arange(bin_size, 1 + bin_size, bin_size)\n",
    "\n",
    "    accuracies = []\n",
    "    confidences = []\n",
    "    bin_lengths = []\n",
    "\n",
    "    for conf_thresh in upper_bounds:\n",
    "        acc, avg_conf, len_bin = compute_acc_bin(conf_thresh - bin_size, conf_thresh, conf, pred, true)\n",
    "        accuracies.append(acc)\n",
    "        confidences.append(avg_conf)\n",
    "        bin_lengths.append(len_bin)\n",
    "\n",
    "    return accuracies, confidences, bin_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram(accs, confs, ece, M=15, name=\"\"):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes()\n",
    "\n",
    "    accs = np.array(accs)\n",
    "    confs = np.array(confs)\n",
    "\n",
    "    bin_size = 1 / M\n",
    "    positions = np.arange(0 + bin_size / 2, 1 + bin_size / 2, bin_size)\n",
    "\n",
    "    gap_below_diagonal = confs - accs\n",
    "    gap_below_diagonal[gap_below_diagonal < 0] = 0\n",
    "\n",
    "    gap_above_diagonal = accs - confs\n",
    "    gap_above_diagonal[gap_above_diagonal < 0] = 0\n",
    "\n",
    "    # Bars with outputs\n",
    "    accs_plt = ax.bar(positions, accs, width=bin_size, edgecolor=\"black\", color=\"#00A4CCFF\", label=\"Outputs\", zorder=2)\n",
    "    confs_plt = ax.bar(\n",
    "        positions, confs, width=bin_size, edgecolor=\"red\", color=\"#F95700FF\", alpha=0.0, linewidth=2, zorder=3\n",
    "    )\n",
    "    gap_above_diagonal_plt = ax.bar(\n",
    "        positions,\n",
    "        gap_above_diagonal,\n",
    "        width=bin_size,\n",
    "        edgecolor=\"red\",\n",
    "        color=\"red\",\n",
    "        label=\"Gap\",\n",
    "        alpha=0.3,\n",
    "        linewidth=2,\n",
    "        bottom=confs,\n",
    "        zorder=3,\n",
    "    )\n",
    "    gap_below_diagonal_plt = ax.bar(\n",
    "        positions,\n",
    "        gap_below_diagonal,\n",
    "        width=bin_size,\n",
    "        edgecolor=\"red\",\n",
    "        color=\"red\",\n",
    "        alpha=0.3,\n",
    "        linewidth=2,\n",
    "        bottom=accs,\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Line plot with center line.\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=5)\n",
    "    ax.legend(handles=[gap_above_diagonal_plt, accs_plt], prop={\"size\": 30})\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.xticks(fontsize=23, rotation=90)\n",
    "    plt.yticks(fontsize=23, rotation=0)\n",
    "    props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "    ax.text(\n",
    "        0.41,\n",
    "        0.1,\n",
    "        \"ECE Error = {:2.2f}%\".format(ece),\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=30,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=props,\n",
    "    )\n",
    "    ax.set_xlabel(\"Confidence\", fontsize=30, color=\"black\")\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=30, color=\"black\")\n",
    "    plt.savefig(f\"{name}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15\n",
    "bin_size = 1 / M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_confs, y_preds, y_trues = get_output(model, val_loader)\n",
    "ece = ECE(y_confs, y_preds, y_trues, bin_size) * 100\n",
    "accs, confs, len_bins = get_bin_info(y_confs, y_preds, y_trues, bin_size=bin_size)\n",
    "reliability_diagram(accs, confs, ece, M, \"image_net_before_calibration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_confs, y_preds, y_trues = get_output(scaled_model, val_loader)\n",
    "ece = ECE(y_confs, y_preds, y_trues, bin_size) * 100\n",
    "accs, confs, len_bins = get_bin_info(y_confs, y_preds, y_trues, bin_size=bin_size)\n",
    "reliability_diagram(accs, confs, ece, M, \"image_net_after_calibration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
